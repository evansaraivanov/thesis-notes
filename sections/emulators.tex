\chapter{Data Emulators}
As can be seen from the growth-geometry split analysis, an enormous amount of time and computing resources are needed to do a comprehensive analysis of novel cosmological models and extensions to $\Lambda$CDM. There are a growing number of proposed extension theories, and more data sets that can test them. One of the ways to improve the efficiency of data and theory analysis is to create a faster way to compute data vectors when doing MCMC sampling. In this chapter, I will describe how one can use neural networks (NN) to learn this mapping, bypassing the need for expensive programs such as Comsolike.
\section{Neural Networks}
Although the term `neural network' is almost synonymous with magic in the modern world, the concept is very natural from elementary mathematics. There is a basic example: linear regression. 

In linear regression, one considers a model of the form $y=mx+b$. The goal is to find the values of $m$ and $b$ that minimizes the square of the residual
\begin{equation}
	L(y_{i,\mathrm{truth}},y_{i,\mathrm{model}}) = (y_{i,\mathrm{truth}} - y_{i,\mathrm{model}})^2\,.
\end{equation}
In this exceptional case, there is an analytic way to minimize $L$, but in general, it is a highly non-trivial task. Additionally, there are much more complicated data sets one would want to model, such as data vectors in cosmological surveys. The remaining question is: how does one intelligently generalize this type of model to more complicated ones?

We, as humans, have a tool that can do this already: the brain~\cite{noauthor_what_nodate}! The brain is a large system of cells called neurons. Each neuron is connected to others by a synapse, and after accumulating a large enough electric charge, the neuron will activate, sending the electric signal to a connected neuron. 

Using this, one can attempt to construct a general model, starting with the neurons~\ref{fig:neurons}.
\begin{figure}[th]
	\centering
	\begin{tikzpicture}
		\draw (-2,0) circle (5pt);
		\draw (0,0) circle (5pt);
		\draw (2,0) circle (5pt);
		\draw (1,1.41) circle (5pt);
		\draw (-1,1.41) circle (5pt);
		\draw (1,-1.41) circle (5pt);
		\draw (-1,-1.41) circle (5pt);
	\end{tikzpicture}
	\caption{Arrangement of neurons.}
	\label{fig:neurons}
\end{figure}
Next, add in the synapses, the connections between neurons~\ref{fig:neurons_synapse}. In general, not every neuron is connected to each other. It should be noted, along each synapse is an activation function. Rather than having a threshold potential like a real synapse, NNs use functions that represent the activation of the neuron. Furthermore, each synapse has a weight that represents the strength of the connection.
\begin{figure}[th]
	\centering
	\begin{tikzpicture}
		\draw (-2,0) -- (0,0);
		\draw (-1,1.41) -- (0,0);
		\draw (-1,1.41) -- (-2,0);
		\draw (-1,-1.41) -- (-2,0);
		\draw (-1,-1.41) -- (1,-1.41);
		\draw (2,0) -- (1,-1.41);
		\draw (1,1.41) -- (-1,1.41);
		\draw (1,1.41) -- (1,-1.41);
		\draw (2,0) -- (-1,-1.41);
		\draw (0,0) -- (1,-1.41);
		\draw (0,0) -- (1,1.41);
		\draw (0,0) -- (2,0);
		\draw (-1,1.41) -- (2,0);
		\draw (1,1.41) -- (-2,0);
		\draw[fill=white] (-2,0) circle (5pt);
		\draw[fill=white] (0,0) circle (5pt);
		\draw[fill=white] (2,0) circle (5pt);
		\draw[fill=white] (1,1.41) circle (5pt);
		\draw[fill=white] (-1,1.41) circle (5pt);
		\draw[fill=white] (1,-1.41) circle (5pt);
		\draw[fill=white] (-1,-1.41) circle (5pt);
	\end{tikzpicture}
	\caption{Neurons with synapses.}
	\label{fig:neurons_synapse}
\end{figure}
Next, define which neurons receive an external signal, and which give a signal to the outside~\ref{fig:neurons_synapse_io}. These act as the input and output.
\begin{figure}[th]
	\centering
	\begin{tikzpicture}
		\draw (-2,0) -- (0,0);
		\draw (-1,1.41) -- (0,0);
		\draw (-1,1.41) -- (-2,0);
		\draw (-1,-1.41) -- (-2,0);
		\draw (-1,-1.41) -- (1,-1.41);
		\draw (2,0) -- (1,-1.41);
		\draw (1,1.41) -- (-1,1.41);
		\draw (1,1.41) -- (1,-1.41);
		\draw (2,0) -- (-1,-1.41);
		\draw (0,0) -- (1,-1.41);
		\draw (0,0) -- (1,1.41);
		\draw (0,0) -- (2,0);
		\draw (-1,1.41) -- (2,0);
		\draw (1,1.41) -- (-2,0);
		\draw (-3,0) -- (-2,0);
		\draw (-3,0) node[anchor=east] {In};
		\draw (1,1.41) -- (2,2.41);
		\draw (2,0) -- (3,0);
		\draw (3,0) node[anchor=west] {Out};
		\draw (2,2.41) node[anchor=south west] {Out};
		\draw[fill=white] (-2,0) circle (5pt);
		\draw[fill=white] (0,0) circle (5pt);
		\draw[fill=white] (2,0) circle (5pt);
		\draw[fill=white] (1,1.41) circle (5pt);
		\draw[fill=white] (-1,1.41) circle (5pt);
		\draw[fill=white] (1,-1.41) circle (5pt);
		\draw[fill=white] (-1,-1.41) circle (5pt);
	\end{tikzpicture}
	\caption{Neurons, synapses, and the input/output.}
	\label{fig:neurons_synapse_io}
\end{figure}
Finally, there is a choice to make in the activation function. This is the chance to greatly increase the modelling capability of NNs. The only requirement is that the function is continuous; thus one can use non-linear activation functions, allowing modelling of highly non-linear outputs. 

The goal is to define a loss function that will be minimized (in the linear regression, the loss is the square of the residual). The loss is a function of the output of the NN, and the output is parameterized by the parameters of the NN. Thus, if $y=\phi(\alpha_i,x)$ represents the output of the neural network with parameters $\alpha_i$ and input $x$, the loss function is, in fact, parameterized by the neural network as well.
\begin{equation}
	L(y) = L(\phi(\alpha_i,x)) = \mathfrak{L}(\alpha_i,x)
\end{equation}
Thus, a neural network is, in simple terms, an optimization problem. The issue, however, is that neural networks generally have numerous parameters (thousands, millions, even billions!), so this optimization is difficult in practice. Moreover, what may appear as a simple loss function in terms of the output can become an extremely complicated (and, importantly, non-monotonic) function of $\alpha_i$.

The general method of optimization is called \textit{gradient descent}. The strategy is the following:
\begin{enumerate}
	\item Define an initial set of parameters $\alpha_{i,\mathrm{init}}$.
	\item Split the training data into $n$ batches.
	\item Generate a new set of parameters for each batch, centered around $\alpha_{i,\mathrm{init}}$, denoted $\alpha^{n}_{i}$.
	\item Compute the loss for each batch.
	\item Compute the gradient of the neural network.
	\item Update the parameters according to the gradient.
	\item Repeat until the gradient is 0.
\end{enumerate}
This simple algorithm has one glaring flaw: it is susceptible to converging in a local minimum. Ideally, gradient descent will converge to the global minimum of the loss function. The way around this is to add some stochasticity to the gradient descent so that the network does not simply search for zero gradient. Instead, the parameters are updated probabilistically from the gradient.

The gradient is usually computed using \textit{backpropagation}, in which the gradient is computed at each layer and the total gradient is computed according to the chain rule. This presents an issue: if the gradient is small at each layer, the gradient will be small on the order of $\epsilon^{n}$, where $n$ is the number of layers. This can lead to a misleading gradient that vanishes, or a gradient that explodes to large values. This is the problem of \textit{vanishing gradients}, and methods to avoid this are discussed in the next section.

The last issue to mention is that, given numerous parameters, it is possible to fit the training data very well, but the model cannot fit external data sets. This is called \textit{overfitting}. There are many strategies to resolve this. The two used in this project are dropout and L2 regularization. Dropout randomly sets a fraction of the weights to zero, meaning the NN will not be able to rely on individual neurons to get a reliable output. This ensures the full network is being used. L2 regularization penalizes the model for learning large weights, which again means the model relies on particular neurons. This is accomplished by modifying the loss function by adding the L2 norm of the network.
\begin{equation}
	L(y) \mapsto L(y)+\sum_i \alpha_i^2
\end{equation}
In summary, a NN is a directed graph, where each edge has a weight and an activation function, and each node has a value that it passes along to the edge. The inputs are propagated to the output, and the goal is to minimize the loss of the output. It is important to pick a smart loss function depending on one's needs from the NN and a smart activation function depending on the values of each node. The loss function is minimized using stochastic gradient descent. It is also important to implement strategies to avoid overfitting on the training data by penalizing the model for relying too heavily on certain regions of the graph.

Using NNs to compute data vectors is not a new technique~\cite{boruah_accelerating_2022,to_linna_2023}. The previous methods typically use an iterative approach, where they start by training on a Latin hypercube, and use the trained model to generate new training points from an MCMC that are added to the hypercube points. This process is repeated until the loss function passes some threshold. The claim that this method gives satisfactory results on the entire Latin hypercube is somewhat misleading. While the hypercube points remain in the training data, the points become diluted until they have minimal contribution to the training loss. Testing with the emulator in~\cite{boruah_accelerating_2022}, the emulator begins to fail at around $4\sigma$ shifts. Our goal is to greatly improve the region where the emulators are useful. This will allow other scientists to use emulators to analyze real data, where the fiducial cosmology is not known.

\section{Architecture Choices}
As discussed before, the weights, activation functions, and connections are choices one can make when designing a neural network. Together, they comprise the architecture. This section discusses the architecture choices we have made.

Before discussing the graph architecture, we want to highlight the choice of activation function. Before giving any data to the neural network, we do some preprocessing to the data. The first thing we do is normalize the input and output. The input is the cosmological parameters, which we process each parameter to follow a normal distribution:
\begin{equation}
	x^i = \frac{\theta^i - \bar\theta^i}{\sigma_i}\,.
\end{equation}
The output is the cosmic shear data vector. We preprocess this by diagonalizing and normalizing each component,
\begin{equation}
	y^i = \frac{ (P^{-1}d)^i }{\sqrt{(PCP^{-1})^{ii}}}\,,
\end{equation}
where $C$ is the covariance of the data and $P$ is the change of basis matrix to the eigenbasis of $C$. This generally means we will have many negative parameters, and the input and output are both symmetric about 0, and thus we choose to use an antisymmetric activation function so that the symmetry around 0 can be preserved. In our case, we use $\tanh(x)$. Additionally, all of these NNs are feedforward sequential models. Sequential means we separate the neurons into layers, and feedforward means the data moves only from layer $n$ to layer $n+1$ and never backwards.
\subsection{Multi-Layer Perceptron}
The first graph architecture we study in the multi-layer perceptron (the name is adopted from convolutional neural networks `perceiving' image features). In this architecture, a node in a given layer is connected to every node in the following layer, so this network is called `simply connected' (figure~\ref{fig:mlp}).
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
		\draw (0,2) node[anchor=south] {In};
		\draw (-0.5,2) -- (-2,1);
		\draw (-0.5,2) -- (-1,1);
		\draw (-0.5,2) -- (0,1);
		\draw (-0.5,2) -- (1,1);
		\draw (-0.5,2) -- (2,1);
		\draw (0.5,2) -- (-2,1);
		\draw (0.5,2) -- (-1,1);
		\draw (0.5,2) -- (0,1);
		\draw (0.5,2) -- (1,1);
		\draw (0.5,2) -- (2,1);
		%1-2
		\draw (-2,1) -- (-2,0);
		\draw (-2,1) -- (-1,0);
		\draw (-2,1) -- (0,0);
		\draw (-2,1) -- (1,0);
		\draw (-2,1) -- (2,0);
		\draw (-1,1) -- (-2,0);
		\draw (-1,1) -- (-1,0);
		\draw (-1,1) -- (0,0);
		\draw (-1,1) -- (1,0);
		\draw (-1,1) -- (2,0);
		\draw (0,1) -- (-2,0);
		\draw (0,1) -- (-1,0);
		\draw (0,1) -- (0,0);
		\draw (0,1) -- (1,0);
		\draw (0,1) -- (2,0);
		\draw (1,1) -- (-2,0);
		\draw (1,1) -- (-1,0);
		\draw (1,1) -- (0,0);
		\draw (1,1) -- (1,0);
		\draw (1,1) -- (2,0);
		\draw (2,1) -- (-2,0);
		\draw (2,1) -- (-1,0);
		\draw (2,1) -- (0,0);
		\draw (2,1) -- (1,0);
		\draw (2,1) -- (2,0);
		%2-3
		\draw (-2,0) -- (-1,-1);
		\draw (-2,0) -- (0,-1);
		\draw (-2,0) -- (1,-1);
		\draw (-1,0) -- (-1,-1);
		\draw (-1,0) -- (0,-1);
		\draw (-1,0) -- (1,-1);
		\draw (0,0) -- (-1,-1);
		\draw (0,0) -- (0,-1);
		\draw (0,0) -- (1,-1);
		\draw (1,0) -- (-1,-1);
		\draw (1,0) -- (0,-1);
		\draw (1,0) -- (1,-1);
		\draw (2,0) -- (-1,-1);
		\draw (2,0) -- (0,-1);
		\draw (2,0) -- (1,-1);
		\draw (0,-1) node[anchor=north] {Out};
		%layer in to 1
		\draw[fill=white] (-0.5,2) circle (5pt);
		\draw[fill=white] (0.5,2) circle (5pt);
		\draw[fill=white] (-2,1) circle (5pt);
		\draw[fill=white] (-1,1) circle (5pt);
		\draw[fill=white] (0,1) circle (5pt);
		\draw[fill=white] (1,1) circle (5pt);
		\draw[fill=white] (2,1) circle (5pt);
		\draw[fill=white] (-2,0) circle (5pt);
		\draw[fill=white] (-1,0) circle (5pt);
		\draw[fill=white] (0,0) circle (5pt);
		\draw[fill=white] (1,0) circle (5pt);
		\draw[fill=white] (2,0) circle (5pt);
		\draw[fill=white] (-1,-1) circle (5pt);
		\draw[fill=white] (0,-1) circle (5pt);
		\draw[fill=white] (1,-1) circle (5pt);
	\end{tikzpicture}
	\caption{A multi-layer perceptron.}
	\label{fig:mlp}
\end{figure}
\subsection{Residual Network}
As mentioned above, there is an issue of vanishing gradients with deep neural networks. To resolve this, one can add the input and output together so that gradient information is allowed to skip layers~\cite{he_deep_2015}(figure~\ref{fig:resblock}). Suppose we have a block of layers parameterized by $\alpha_i$, and its input is from another layer, parameterized by $\beta_i$, so that $w=\chi(\beta_i,x)$ for an input $x$. Then the block of layers, whose map is denoted $\phi$, will be
\begin{equation}
	\phi(\alpha_i,\chi(\beta_i,x)) = \psi(\alpha_i,\beta_i,x)+\chi(\beta_i,x)\,.
\end{equation}
This section of the model is going to attempt to learn
\begin{equation}
	\psi(\alpha_i,\beta_i,x)-\chi(\beta_i,x)\,.
\end{equation}
This allows gradients with respect to the parameters $\beta_i$ to propagate to the output without the multiplication by the gradients of $\alpha_i$, which usually solves the vanishing gradient problem. This is called a \textit{residual neural network} (ResNet).
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
		\draw (0,2.75) node {\footnotesize $\chi(\beta_i,x)$};
		\draw (-2,2.5) -- (2,2.5) -- (2,3) -- (-2,3) -- (-2,2.5);
		\draw (-1.5,2.5) -- (-1.5,2);
		\draw (-0.5,2.5) -- (-0.5,2);
		\draw (0.5,2.5) -- (0.5,2);
		\draw (1.5,2.5) -- (1.5,2);
		%in
		\draw (-1.5,2) -- (-1.5,1);
		\draw (-1.5,2) -- (-0.5,1);
		\draw (-1.5,2) -- (0.5,1);
		\draw (-1.5,2) -- (1.5,1);
		\draw (-0.5,2) -- (-1.5,1);
		\draw (-0.5,2) -- (-0.5,1);
		\draw (-0.5,2) -- (0.5,1);
		\draw (-0.5,2) -- (1.5,1);
		\draw (0.5,2) -- (-1.5,1);
		\draw (0.5,2) -- (-0.5,1);
		\draw (0.5,2) -- (0.5,1);
		\draw (0.5,2) -- (1.5,1);
		\draw (1.5,2) -- (-1.5,1);
		\draw (1.5,2) -- (-0.5,1);
		\draw (1.5,2) -- (0.5,1);
		\draw (1.5,2) -- (1.5,1);
		%%% hidden
		\draw (-1.5,1) -- (-1.5,0);
		\draw (-1.5,1) -- (-0.5,0);
		\draw (-1.5,1) -- (0.5,0);
		\draw (-1.5,1) -- (1.5,0);
		\draw (-0.5,1) -- (-1.5,0);
		\draw (-0.5,1) -- (-0.5,0);
		\draw (-0.5,1) -- (0.5,0);
		\draw (-0.5,1) -- (1.5,0);
		\draw (0.5,1) -- (-1.5,0);
		\draw (0.5,1) -- (-0.5,0);
		\draw (0.5,1) -- (0.5,0);
		\draw (0.5,1) -- (1.5,0);
		\draw (1.5,1) -- (-1.5,0);
		\draw (1.5,1) -- (-0.5,0);
		\draw (1.5,1) -- (0.5,0);
		\draw (1.5,1) -- (1.5,0);
		%%%
		\draw (-1.5,-0.5) -- (-1.5,0);
		\draw (-0.5,-0.5) -- (-0.5,0);
		\draw (0.5,-0.5) -- (0.5,0);
		\draw (1.5,-0.5) -- (1.5,0);
		%%%
		\draw (-2,-0.5) -- (2,-0.5) -- (2,-1) -- (-2,-1) -- (-2,-0.5);
		\draw (-2,-2) -- (2,-2) -- (2,-2.5) -- (-2,-2.5) -- (-2,-2);
		\draw (0,-0.75) node {\footnotesize $\psi(\alpha_i,\beta_i,x)$};
		\draw (0,-1) -- (0,-2);
		\draw (2,2.75) to[out=0,in=90] (3,1.75) -- (3,-0.5) to[out=-90,in=0] (2,-1.5) -- (0,-1.5);
		\draw (0,-2.25) node {\footnotesize $\phi(\alpha_i,\chi(\beta_j,x))$};
		\draw (3,0.83) node[anchor=west] {\footnotesize $\chi(\beta_i,x)$};
		%layer in to 1
		\draw[fill=white] (-1.5,2) circle (5pt);
		\draw[fill=white] (-0.5,2) circle (5pt);
		\draw[fill=white] (0.5,2) circle (5pt);
		\draw[fill=white] (1.5,2) circle (5pt);
		\draw[fill=white] (-1.5,1) circle (5pt);
		\draw[fill=white] (-0.5,1) circle (5pt);
		\draw[fill=white] (0.5,1) circle (5pt);
		\draw[fill=white] (1.5,1) circle (5pt);
		\draw[fill=white] (-1.5,0) circle (5pt);
		\draw[fill=white] (-0.5,0) circle (5pt);
		\draw[fill=white] (0.5,0) circle (5pt);
		\draw[fill=white] (1.5,0) circle (5pt);
		\draw[fill=white] (0,-1.5) circle (5pt);
		\draw (0,-1.5) node {\footnotesize{$+$}};
	\end{tikzpicture}
	\caption{A residual block.}
	\label{fig:resblock}
\end{figure}
\subsection{Bottlenecked Residual Network}
The ResNet allows us to create much deeper networks, but the number of parameters stays the same as the linear model, making ResNets difficult to train (and especially the minimum of two linear layers. We can make a small modification to the ResNet that can resolve this, where there is an additional layer and the number of dimensions reduces in the middle (figure~\ref{fig:resbottle-block}). This forces the neural network to prioritize certain features and ignore ones that are not as necessary. Due to the shape this architecture makes, it is referred to as a \textit{bottlenecked residual network} (ResBottle) (figure~\ref{fig:resbottle-block}).
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
		\draw (1.5,1) -- (0.5,0);
		\draw (1.5,1) -- (-0.5,0);
		\draw (0.5,1) -- (0.5,0);
		\draw (0.5,1) -- (-0.5,0);
		\draw (-0.5,1) -- (0.5,0);
		\draw (-0.5,1) -- (-0.5,0);
		\draw (-1.5,1) -- (0.5,0);
		\draw (-1.5,1) -- (-0.5,0);
		\draw (-0.5,0) -- (-0.5,-1);
		\draw (-0.5,0) -- (0.5,-1);
		\draw (0.5,0) -- (-0.5,-1);
		\draw (0.5,0) -- (0.5,-1);
		\draw (1.5,-2) -- (0.5,-1);
		\draw (1.5,-2) -- (-0.5,-1);
		\draw (0.5,-2) -- (0.5,-1);
		\draw (0.5,-2) -- (-0.5,-1);
		\draw (-0.5,-2) -- (0.5,-1);
		\draw (-0.5,-2) -- (-0.5,-1);
		\draw (-1.5,-2) -- (0.5,-1);
		\draw (-1.5,-2) -- (-0.5,-1);
		%%%
		\draw (-1.5,1.5) -- (-1.5,1);
		\draw (-0.5,1.5) -- (-0.5,1);
		\draw (0.5,1.5) -- (0.5,1);
		\draw (1.5,1.5) -- (1.5,1);
		\draw (-1.5,-2.5) -- (-1.5,-2);
		\draw (-0.5,-2.5) -- (-0.5,-2);
		\draw (0.5,-2.5) -- (0.5,-2);
		\draw (1.5,-2.5) -- (1.5,-2);
		\draw (-2,-2.5) -- (2,-2.5) -- (2,-3) -- (-2,-3) -- (-2,-2.5);
		\draw (-2,1.5) -- (2,1.5) -- (2,2) -- (-2,2) -- (-2,1.5);
		%%%
		\draw (0,-4) -- (0,-3);
		\draw (-2,-4.5) -- (2,-4.5) -- (2,-4) -- (-2,-4) -- (-2,-4.5);
		%%%
		\draw[fill=white] (1.5,1) circle (5pt);
		\draw[fill=white] (0.5,1) circle (5pt);
		\draw[fill=white] (-0.5,1) circle (5pt);
		\draw[fill=white] (-1.5,1) circle (5pt);
		%%%
		\draw[fill=white] (0.5,0) circle (5pt);
		\draw[fill=white] (-0.5,0) circle (5pt);
		%%%
		\draw[fill=white] (0.5,-1) circle (5pt);
		\draw[fill=white] (-0.5,-1) circle (5pt);
		%%%
		\draw[fill=white] (1.5,-2) circle (5pt);
		\draw[fill=white] (0.5,-2) circle (5pt);
		\draw[fill=white] (-0.5,-2) circle (5pt);
		\draw[fill=white] (-1.5,-2) circle (5pt);
		%%%
		\draw (2,1.75) to[out=0,in=90] (3,0.75) -- (3,-2.5) to[out=-90,in=0] (2,-3.5) -- (0,-3.5);
		\draw[fill=white] (0,-3.5) circle (5pt);
		\draw (0,-3.5) node {\footnotesize $+$};
		\draw (0,1.75) node {\footnotesize $\chi(\beta_i,x)$};
		\draw (0,-2.75) node {\footnotesize $\psi(\alpha_i,\beta_j,x)$};
		\draw (0,-4.25) node {\footnotesize $\phi(\alpha_i, \chi(\beta_j,x))$};
		\draw (3,-1.12) node[anchor=west] {\footnotesize $\chi(\beta_i,x)$};
	\end{tikzpicture}
	\caption{A bottlenecked residual block.}
	\label{fig:resbottle-block}
\end{figure}
In the examples drawn (figures~\ref{fig:resblock} and~\ref{fig:resbottle-block}), there are 40 parameters in the ResNet, but only 28 parameters in the ResBottle. This allows us to either simplify the model, add more layers, or make the layers much wider without blowing up the number of parameters that need to be trained.
\subsection{Attention}
This will be the most complicated model to test. If one examines the structure of the data vector, there is a certain order to the data and correlation between the components. The data vectors are separated into redshift bins, and within each redshift bin are smaller bins representing different scales. Thus, a NN that can learn sequences may be useful. Fortunately, there are NNs encountered every day that use sequences: chatbots~\cite{ruby_how_2023}! The basis for this is usually attention networks~\cite{vaswani_attention_2017}. These work by examining dot products between vectors so that the output is directly related to the angle between the two vectors.

Suppose there is a given sequence of input vectors, $x=(x_1,x_2,\ldots,x_n)$, then one can construct dot products of each $x_i$ and use them as coefficients for the output.
\begin{equation}
	y_k = (x_i \cdot x_j)x_k
\end{equation}
One can then introduce a generalized dot product using linear transformations. That is, one can introduce the transformation matrices $W_Q$, $W_K$, $W_V$. Let $q_i = W_Q x_i$, $k_i = W_K x_i$, and $v_i = W_V x_i$. Then the dot product can be written as 
\begin{equation}
	y_k = (q_i^T k_j)v_k\,.
\end{equation}
Finally, one can introduce a non-linearity, which also keeps the output sequence $y$ normalized, by introducing the \textit{softmax} function,
\begin{equation}
	y_k = \mathrm{softmax}(q_i^T k_j / \sqrt{d})v_k\,,
\end{equation}
where the softmax function is an exponential normalization, given by
\begin{equation}
	\mathrm{softmax}(W)_i = \frac{e^{w_i}}{e^{\sum_i{w_i}}}\,,
\end{equation}
and $d$ is the embedding dimension (i.e $W_K\in \mathbb{R}^{d_x\times d}$).
The linear matrices $W_Q$, $W_K$, and $W_V$, contain learnable components. Each output $y_i$ is then passed to its own MLP. We also keep the skip connections for residual learning. The attention together with the MLPs and skip connections is called a \textit{transformer block}, drawn in figure~\ref{fig:attention}.
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[scale=0.9]
		%%curved skip lines
		\draw (8,2.5) to[out=0,in=90] (9,1.5) -- (9,-3.5) to[out=-90,in=0] (8,-4.5) -- (4.75,-4.5);
		\draw (8,-5.5) to[out=0,in=90] (9,-6.5) -- (9,-8.5) to[out=-90,in=0] (8,-9.5) -- (4.75,-9.5);
		%%input
		\draw (0,2) -- (8,2) -- (8,3) -- (0,3) -- (0,2);
		\draw (4,2.5) node {\footnotesize $x=(x_1,x_2,\dots,x_n)$};
		%%next!
		%\draw (1,4) node[anchor=south] {\footnotesize $x_i$};
		\draw (1,2) -- (1,1.5);
		\draw (1,1) node {\footnotesize $W_Q$};
		\draw (0,1.5) -- (0,0.5) -- (2,0.5) -- (2,1.5) -- (0,1.5);
		%\draw (4,4) node[anchor=south] {\footnotesize $x_j$};
		\draw (4,2) -- (4,1.5);
		\draw (4,1) node {\footnotesize $W_K$};
		\draw (3,1.5) -- (3,0.5) -- (5,0.5) -- (5,1.5) -- (3,1.5);
		%%next!
		\draw (1,0.5) -- (1,0);
		\draw (4,0.5) -- (4,0);
		\draw (5,-1) -- (0,-1) -- (0,0) -- (5,0) --  (5,-1);
		\draw (2.5,-0.5) node {\footnotesize $q_i^T k_j$};
		%%next!
		%\draw (7,4) node[anchor=south] {\footnotesize $x_k$};
		\draw (7,2) -- (7,1.5);
		\draw (7,1) node {\footnotesize $W_V$};
		\draw (6,1.5) -- (6,0.5) -- (8,0.5) -- (8,1.5) -- (6,1.5);
		%%next!
		\draw (2.5,-1) -- (2.5,-1.5);
		\draw (1.5,-1.5) -- (1.5,-2.5) -- (3.5,-2.5) -- (3.5,-1.5) -- (1.5,-1.5);
		\draw (2.5,-2) node {\footnotesize softmax};
		%%next!
		\draw (7,0.5) -- (7,-3);
		\draw (2.5,-2.5) -- (2.5,-3);
		\draw (1.5,-3) -- (1.5,-4) -- (8,-4) -- (8,-3) -- (1.5,-3);
		\draw (4.75,-3.5) node {\footnotesize $\mathrm{softmax}(q_i^T k_j)v_k$};
		%%skips!
		\draw (4.75,-4) -- (4.75,-5);
		\draw[fill=white] (4.75,-4.5) circle (5pt);
		\draw (4.75,-4.5) node {\footnotesize $+$};
		%mlps
		\draw (1.5,-5) -- (1.5,-6) -- (8,-6) -- (8,-5) -- (1.5,-5);
		\draw (4.75,-5.5) node {\footnotesize $\mathrm{softmax}(q_i^T k_j)v_k + x_k$};
		\draw (2,-6) -- (2,-6.5);
		\draw (1.5,-6.5) -- (1.5,-7.5) -- (2.5,-7.5) -- (2.5,-6.5) -- (1.5,-6.5);
		\draw (2,-7) node {\footnotesize MLP};
		\draw (3.5,-6) -- (3.5,-6.5);
		\draw (3,-6.5) -- (3,-7.5) -- (4,-7.5) -- (4,-6.5) -- (3,-6.5);
		\draw (3.5,-7) node {\footnotesize MLP};
		\draw (5.5,-7) node {$\hdots$};
		\draw (7.5,-6) -- (7.5,-6.5);
		\draw (7,-6.5) -- (7,-7.5) -- (8,-7.5) -- (8,-6.5) -- (7,-6.5);
		\draw (7.5,-7) node {\footnotesize MLP};
		%%last skip
		\draw (7.5,-7.5) -- (7.5,-8);
		\draw (3.5,-7.5) -- (3.5,-8);
		\draw (2,-7.5) -- (2,-8);
		\draw (1.5,-8) -- (1.5,-9) -- (8,-9) -- (8,-8) -- (1.5,-8);
		\draw (4.75,-8.5) node {\footnotesize $\mathcal{F}(\mathrm{softmax}(q_i^T k_j)v_k + x_k)$};
		\draw (4.75,-9) -- (4.75,-10);
		\draw (1.5,-10) -- (1.5,-11) -- (8,-11) -- (8,-10) -- (1.5,-10);
		\draw[fill=white] (4.75,-9.5) circle (5pt);
		\draw (4.75,-9.5) node {\footnotesize $+$};
		\draw (4.75,-10.5) node {\footnotesize $y = \mathcal{F}(\mathrm{softmax}(q_i^T k_j)v_k + x_k)+x_k$};
	\end{tikzpicture}
	\caption{A transformer block}
	\label{fig:attention}
\end{figure}

\section{Results}
We present the results in two sections: the first will be about the training and testing results, and the second comparing between a \textsc{Cocoa} chain and an emulated chain. For training and testing, we use the "train, validate, test" approach. We generate three completely independent chains, one is only for training the model, and is a high-temperature chain with data well beyond the expected range. The second is used to validate that the model is not overfitting as the training proceeds. The last set is another high-temperature chain (but not as high as the training) which is used to test the model as if it were being used as normal. The testing involves reintroducing specific analysis choices, such as the scale cut.

When deciding on which model to use, we want to look at several aspects:
\begin{itemize}
	\item The number of parameters. The more parameters we have, the longer it takes to train, and the longer it takes to evaluate. This will make it difficult for groups without top-of-the-line equipment.
	\item The $\Delta\chi^2$ between the emulator and accepted computational methods. Many experiments have a $\chi^2$ requirement for their emulators; for example, DES requires $\Delta\chi^2<0.3$ between any approximations and \textsc{CosmoLike}.
	\item Generalizability. As we increase the volume covered in the parameter space, which models can keep up with the growing modelling volume? Which models can handle more input parameters (which many models need, such as early dark energy with $\sim10$ extra parameters).
\end{itemize}
These get analyzed on the test data set. Because of the second point above, we define the loss function as the difference in $\chi^2$ between the emulator and \textsc{CosmoLike}. This allows us to capture the covariance of the data.
\subsection{Training and Testing}
We train the emulator by using a rough Gaussian approximation on the cosmological parameters. The benefit is that running MCMC chains on a simple Gaussian without the need for \textsc{CosmoLike} is extremely fast. Given the samples, the data vector calculation is trivially parallelizable, so the entire process is significantly faster than running an MCMC in \textsc{CosmoLike}. 

% First, we examine the training and validation loss at each epoch. The training loss is the average loss of all batches in an epoch. Figure~\ref{fig:train_loss_resnet} displays the training loss of a ResNet model with 3 ResNet blocks and 256 neurons in each hidden layer. In this case, we can see a consistent decline in both the training and validation loss. Additionally, we can see the points where the learning rate decreases (see epoch 50 and epoch 85 in figure~\ref{fig:train_loss_resnet}) which occur when validation loss plateaus, following the expected behavior. This demonstrates that we are likely not overfitting the training data.
% \begin{figure}[tb]
% 	\centering
% 	\includegraphics[width=0.9\textwidth]{plots/losses_resnet_3_256.pdf}
% 	\caption{Loss of a ResNet model with 3 ResNet blocks and 256 neurons in each layer.}
% 	\label{fig:train_loss_resnet}
% \end{figure}
% Using a trained model, we evaluate $\Delta\chi^2$ on the test data, a $T=8$ Gaussian approximated chain (figure~\ref{fig:testing_loss}). As we can see, our model is able to generalize well to a $T=8$ chain. 
% \begin{figure}[tb]
% 	\centering
% 	\includegraphics[width=0.9\textwidth]{plots/cosmic_shear_resnet_nlayer_3_intdim_256.pdf}
% 	\caption{Testing loss on a $T=8$ chain.}
% 	\label{fig:testing_loss}
% \end{figure}
First, we tested the different architectures by training them on a $T=16$ chain. The testing was done on a $T=8$ chain. we find that the ResNet is able to consistently perform the best of the three basic architectures. The number of layers and width of the layers makes minimal difference. An interesting result comes from the wide MLP architectures, where the testing loss suddenly spikes. This is likely due to either vanishing gradients or some poorly chosen hyperparameters of the model. Regardless of the issue, this instability is not desirable, as we would like to provide a framework for others to build emulators of their own, and fine-tuning hyperparameters adds significant difficulty to this process. The ResBottle consistently performs worse than the others, meaning the information the model learns in necessary to correctly model the final data vector. This is perhaps unsurprising for our relatively small models, but if one needs to create much wider networks, the ResBottle may be the only way to keep the number of parameters reasonable. Nevertheless, the ResBottle has a satisfactory performance overall.
% \begin{figure}[htb]
% 	\centering
% 	\begin{subfigure}[b]{0.4\textwidth}
% 		\includegraphics[width=\textwidth]{plots/model_table_mlp.pdf}
% 		\caption{MLP average loss}
% 		\label{fig:mlp_table}
% 	\end{subfigure}
% 	\begin{subfigure}[b]{0.4\textwidth}
% 		\includegraphics[width=\textwidth]{plots/model_table_resnet.pdf}
% 		\caption{ResNet average loss}
% 		\label{fig:resnet_table}
% 	\end{subfigure}
% 	\begin{subfigure}[b]{0.8\textwidth}
% 		\includegraphics[width=\textwidth]{plots/model_table_resbottle.pdf}
% 		\caption{ResBottle average loss}
% 		\label{fig:resbottle_table}
% 	\end{subfigure}
% 	\label{fig:model_tables}
% \end{figure}
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{plots/avg_chi2_v_n_params_new.pdf}
	\caption{Average $\Delta\chi^2$ as a function of the number of parameters for each architecture.}
	\label{fig:avg_chi2_nparams}
\end{figure}

Additionally, we can test how of a broad of a region in parameter space we can train on and still get satisfactory performance. We do this by scaling the covariance by a temperature $T$, i.e $C\mapsto TC$. We noticed the MLP, ResNet, and ResBottle models fail to train on a $T=128$ chain (figure~\ref{fig:avg_chi2_nparams_t128}).
\begin{figure}[!tb]
	\centering
	\includegraphics[width=0.8\textwidth]{plots/avg_chi2_v_n_params_T64.pdf}
	\caption{Average $\Delta\chi^2$ of a $T=64$ chain using emulators trained on a $T=128$ chain.}
	\label{fig:avg_chi2_nparams_t128}
\end{figure} 
We added the transformer block to the $T=128$ training to see the gain in performance attributed to the complex model. We find adding a transformer block to the end of the 3 ResBlock model (figure~\ref{fig:full_arch}) can give satisfactory performance when training on $T=128$ and testing on $T=64$ chains.
\begin{figure}[!tb]
	\centering
	\includegraphics[width=0.8\textwidth]{plots/T64_attention_stuff.pdf}
	\caption{Marginalized distribution of $\Delta\chi^2$ of a $T=64$ chain using the model in figure~\ref{fig:full_arch} trained on a $T=128$ chain.}
	\label{fig:testing_attention_t128}
\end{figure}

\begin{figure}[!tb]
	\centering
	\includegraphics[width=0.8\textwidth]{plots/Untitled Diagram.drawio-5.pdf}
	\caption{Full NN architecture used for training on high-temperature chains. Comprised of three ResBlocks of size 256, and a transformer block with 8 heads, each with a size of 128.}
	\label{fig:full_arch}
\end{figure}

This demonstrates the importance of making an intelligent choice for emulator architecture. By implementing the self attention, we were able to greatly increase the amount of the $\Lambda$CDM parameter space covered by the emulator. The additional volume allows one to use emulators, even on real data, where the data vectors can be shifted greatly from things such as baryon contamination. We would like to continue this to see how this architecture will hold for more complex extensions to $\Lambda$CDM, such as early dark energy, which adds $\sim10$ parameters. We are also working with a team on using this architecture to model much larger data vectors.

\section{Application to Tension Calibration}
In this section, I will apply the use of emulators to do a first study of tension metric error. By shifting the parameters and generating noise realizations on the data, we can get a sense of how the error in the data propagates to error in the tension. This will provide additional information as to which tension metrics perform best, as well as adding context to how tension may vary given new measurements. Additionally, given that $n_\sigma=0$ means no tension, the results of the tension metrics will allow us to gauge how reasonable it may be for the tension to be resolved by additional identical measurements.

We will be shifting the LSST fiducial cosmology to $\pm5\sigma$ in both the $\sigma_8$ and $\Omega_m$ directions. At each of these shifts, we will compute 256 noise realizations on LSST cosmic shear and Planck TTTEEE using their respective data covariance. Finally, the tension will be computed using the methods described in section~\ref{sec:tension_metrics}. Due to time constraints, the chains for the shifts will not be presented in this thesis. We will, however, demonstrate this method using the fiducial cosmologies of LSST and Planck.

The autocorrelation $\tau$ for the MCMC sampler was measured to be $\tau\sim1000$, thus we need to run the chain for about 50000 steps ($50\tau$) using the ensemble sampler in $\textsc{Emcee}$. Each chain has 120 walkers, resulting in a total runtime of just 8 hours on a single 40 core CPU. The joint chains used for $Q_{\mathrm{UDM}}$ were run for just 5000 steps, since that is sufficient for the mean and covariance to converge. 

We generate noise realizations for Planck around the Planck 2018 baseline low-$\ell$ TTTEEE best-fit cosmology~\cite{noauthor_planck_2018}. We utilize \textsc{COSMOPOWER} to emulate the CMB power spectra~\cite{mancini_cosmopower_2022,prince_data_2019}. The Planck chains are run with 120 walkers for 50000. Although the autocorrelation is much lower than the LSST autocorrelation, there needs to be enough data in the tails to accurately estimate the tension.
\begin{figure}[tb]
	\centering
	\includegraphics[width=0.8\textwidth]{plots/metrics_new.pdf}
	\caption{Parameter-based tension metrics with error.}
	\label{fig:tension_metrics_error}
\end{figure}

We see that, up to noise realizations, the different tension metrics are nearly indistinguishable. Both Planck and LSST are nearly Gaussian, but for more non-Gaussian posteriors (such as DES), the difference between the parameter difference and other metrics may be more significant. We also see that the tension can be reduced by the experimental noise, but cannot be reduced to 0. It remains to compute the tension for noise realizations on the cosmologies with shifted $\Omega_m$ and $\sigma_8$.

The result for $Q_{\mathrm{UDM}}$ is significantly lower than the other metrics. This is likely because it makes the assumption that the posteriors are Gaussian, whereas the tension can hide in the non-Gaussian parts of the posterior. Additionally, the error bar on $Q_{\mathrm{DMAP}}$ is much smaller. This requires some investigation into the distribution of the maximum a posteriori of the noise realizations. While noise realizations on the data generate large shifts in the posterior, the same is not necessarily true for the maximum a posteriori.

\section{Conclusion}
We are able to implement a NN emulator that works on temperatures as high as 128 by using the ChatGPT inspired transformer block. Using the emulators, we can ensure that the posteriors are robust for noise realizations on the data. In fact, using a temperature of 128 ensures the emulator should be robust up to $10\sigma$ shifts in the data. Using these noise realizations, we can compute the error on the tension metrics.

A future study will be done creating emulators for different extension models which push the number of input parameters, as well as different data sets which push the number of output parameters. Using this baseline framework, we can also rapidly analyze smaller extensions, such as the geometry and growth split detailed in chapter 4.








